import pickle
import numpy as np
from tqdm import tqdm
import multiprocessing
import os

def cosine_similarity(u, v):
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå®‰å…¨å¤„ç†é›¶å‘é‡æƒ…å†µ
    """
    norm_u = np.linalg.norm(u)
    norm_v = np.linalg.norm(v)
    
    # å¦‚æœä»»ä¸€å‘é‡ä¸ºé›¶å‘é‡ï¼Œè¿”å›0ç›¸ä¼¼åº¦
    if norm_u == 0 or norm_v == 0:
        return 0.0
    
    return np.dot(u, v) / (norm_u * norm_v)

def find_most_similar_embedding(target_emb, umls_ent_emb):
    max_similarity = -1
    max_index = None
    for idx, umls_emb in enumerate(umls_ent_emb):
        similarity = cosine_similarity(target_emb, umls_emb)
        if similarity > max_similarity:
            max_similarity = similarity
            max_index = idx
    if max_similarity > SIMILARITY_THRESHOLD:
        return max_index
    else:
        return None

def process_chunk(chunk, id2emb, umls_ent_emb, output_dict, process_id=None):
    # ç®€åŒ–å¤„ç†ï¼Œä¸åœ¨å­è¿›ç¨‹ä¸­æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œé¿å…æ˜¾ç¤ºæ··ä¹±
    for key in chunk:
        output_dict[key] = find_most_similar_embedding(id2emb[key], umls_ent_emb)

def sequential_mapping(id2emb, umls_ent_emb, desc="Processing"):
    """ä½¿ç”¨å•è¿›ç¨‹å¤„ç†ï¼Œé¿å…å¤§æ–‡ä»¶å¤šæ¬¡åŠ è½½å¯¼è‡´çš„å†…å­˜é—®é¢˜"""
    keys = list(id2emb.keys())
    output_dict = {}
    
    print(f"\n{desc}: æ€»å…± {len(keys)} ä¸ªé¡¹ç›®ï¼Œä½¿ç”¨å•è¿›ç¨‹å¤„ç†ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰")
    
    # æ˜¾ç¤ºè¿›åº¦
    for key in tqdm(keys, desc=f"{desc}"):
        output_dict[key] = find_most_similar_embedding(id2emb[key], umls_ent_emb)
    
    return output_dict

def parallel_mapping(id2emb, umls_ent_emb, num_processes, desc="Processing"):
    """å¦‚æœæ•°æ®é‡å°ï¼Œä½¿ç”¨å¤šè¿›ç¨‹ï¼›å¦‚æœæ•°æ®é‡å¤§ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å•è¿›ç¨‹"""
    # æ£€æŸ¥umls_ent_embçš„å¤§å°ï¼Œå¦‚æœå¤ªå¤§åˆ™ä½¿ç”¨å•è¿›ç¨‹
    umls_size_mb = len(umls_ent_emb) * len(umls_ent_emb[0]) * 8 / (1024 * 1024)  # ä¼°ç®—å¤§å°
    
    if umls_size_mb > 500:  # å¦‚æœè¶…è¿‡500MBï¼Œä½¿ç”¨å•è¿›ç¨‹
        print(f"æ£€æµ‹åˆ°å¤§å‹åµŒå…¥æ–‡ä»¶ (~{umls_size_mb:.1f}MB)ï¼Œä½¿ç”¨å•è¿›ç¨‹å¤„ç†ä»¥é¿å…å†…å­˜é—®é¢˜")
        return sequential_mapping(id2emb, umls_ent_emb, desc)
    
    # åŸæœ‰çš„å¤šè¿›ç¨‹é€»è¾‘
    keys = list(id2emb.keys())
    chunks = np.array_split(keys, num_processes)
    manager = multiprocessing.Manager()
    output_dict = manager.dict()
    processes = []

    print(f"\n{desc}: æ€»å…± {len(keys)} ä¸ªé¡¹ç›®ï¼Œä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹")
    
    for i, chunk in enumerate(chunks):
        p = multiprocessing.Process(target=process_chunk, args=(chunk, id2emb, umls_ent_emb, output_dict, i))
        processes.append(p)
        p.start()

    # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
    import time
    with tqdm(total=len(keys), desc=f"{desc}") as pbar:
        last_count = 0
        while any(p.is_alive() for p in processes):
            current_count = len(output_dict)
            if current_count > last_count:
                pbar.update(current_count - last_count)
                last_count = current_count
            time.sleep(0.5)  # å‡å°‘æ›´æ–°é¢‘ç‡
        
        # ç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºå®Œæˆ
        final_count = len(output_dict)
        if final_count > last_count:
            pbar.update(final_count - last_count)

    for p in processes:
        p.join()

    return dict(output_dict)

if __name__ == "__main__":
    # è·å–è„šæœ¬ç›®å½•å’Œé¡¹ç›®æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.join(script_dir, "../..")
    data_dir = os.path.join(project_root, "data")
    concept_names_path = os.path.join(project_root, "KG_mapping/umls/concept_names.txt")
    
    # åŸå§‹è·¯å¾„: /home/pj20/GraphCare/KG_mapping/umls/concept_names.txt
    with open(concept_names_path, 'r', encoding='utf-8') as f:
        umls_ent = f.readlines() 
    
    umls_ids = []
    umls_names = []

    for line in umls_ent:
        umls_id = line.split('\t')[0]
        umls_name = line.split('\t')[1][:-1]
        umls_names.append(umls_name)
        umls_ids.append(umls_id)
        
        
    # åŸå§‹è·¯å¾„: /data/pj20/exp_data/umls_ent_emb_.pkl
    with open(os.path.join(data_dir, 'umls_ent_emb_.pkl'), 'rb') as f:
        umls_ent_emb = pickle.load(f)
        
    # åŸå§‹è·¯å¾„: /data/pj20/exp_data/atc3_id2emb.pkl
    with open(os.path.join(data_dir, 'atc3_id2emb.pkl'), 'rb') as f:
        atc3_id2emb = pickle.load(f)
        
    # åŸå§‹è·¯å¾„: /data/pj20/exp_data/ccscm_id2emb.pkl
    with open(os.path.join(data_dir, 'ccscm_id2emb.pkl'), 'rb') as f:
        ccscm_id2emb = pickle.load(f)
        
    # åŸå§‹è·¯å¾„: /data/pj20/exp_data/ccsproc_id2emb.pkl
    with open(os.path.join(data_dir, 'ccsproc_id2emb.pkl'), 'rb') as f:
        ccsproc_id2emb = pickle.load(f)



    ccscm2umls, ccsproc2umls, atc32umls = {}, {}, {}

    # Define a similarity threshold
    SIMILARITY_THRESHOLD = 0.7
    # å‡å°‘è¿›ç¨‹æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
    num_processes = min(8, multiprocessing.cpu_count())  # æœ€å¤šä½¿ç”¨8ä¸ªè¿›ç¨‹
    print(f"ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†")

    # ä»»åŠ¡1: å¤„ç† CCSCM åˆ° UMLS çš„æ˜ å°„
    ccscm_file = os.path.join(data_dir, 'ccscm2umls.pkl')
    if os.path.exists(ccscm_file):
        print("\n=== ä»»åŠ¡ 1/3: CCSCM->UMLS æ˜ å°„æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ ===")
        print(f"âœ“ è·³è¿‡ CCSCM->UMLS æ˜ å°„: {ccscm_file}")
    else:
        print("\n=== ä»»åŠ¡ 1/3: å¤„ç† CCSCM åˆ° UMLS çš„æ˜ å°„ ===")
        ccscm2umls = parallel_mapping(ccscm_id2emb, umls_ent_emb, num_processes, "CCSCM->UMLS")
        
        # ç«‹å³ä¿å­˜ CCSCM æ˜ å°„ç»“æœ
        with open(ccscm_file, 'wb') as f:
            pickle.dump(ccscm2umls, f)
        print(f"âœ“ CCSCM->UMLS æ˜ å°„å·²ä¿å­˜: {len(ccscm2umls)} ä¸ªæ˜ å°„ -> {ccscm_file}")
        del ccscm2umls  # é‡Šæ”¾å†…å­˜
    
    # ä»»åŠ¡2: å¤„ç† CCSPROC åˆ° UMLS çš„æ˜ å°„
    ccsproc_file = os.path.join(data_dir, 'ccsproc2umls.pkl')
    if os.path.exists(ccsproc_file):
        print("\n=== ä»»åŠ¡ 2/3: CCSPROC->UMLS æ˜ å°„æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ ===")
        print(f"âœ“ è·³è¿‡ CCSPROC->UMLS æ˜ å°„: {ccsproc_file}")
    else:
        print("\n=== ä»»åŠ¡ 2/3: å¤„ç† CCSPROC åˆ° UMLS çš„æ˜ å°„ ===")
        ccsproc2umls = parallel_mapping(ccsproc_id2emb, umls_ent_emb, num_processes, "CCSPROC->UMLS")
        
        # ç«‹å³ä¿å­˜ CCSPROC æ˜ å°„ç»“æœ
        with open(ccsproc_file, 'wb') as f:
            pickle.dump(ccsproc2umls, f)
        print(f"âœ“ CCSPROC->UMLS æ˜ å°„å·²ä¿å­˜: {len(ccsproc2umls)} ä¸ªæ˜ å°„ -> {ccsproc_file}")
        del ccsproc2umls  # é‡Šæ”¾å†…å­˜
    
    # ä»»åŠ¡3: å¤„ç† ATC3 åˆ° UMLS çš„æ˜ å°„
    atc3_file = os.path.join(data_dir, 'atc32umls.pkl')
    if os.path.exists(atc3_file):
        print("\n=== ä»»åŠ¡ 3/3: ATC3->UMLS æ˜ å°„æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ ===")
        print(f"âœ“ è·³è¿‡ ATC3->UMLS æ˜ å°„: {atc3_file}")
    else:
        print("\n=== ä»»åŠ¡ 3/3: å¤„ç† ATC3 åˆ° UMLS çš„æ˜ å°„ ===")
        atc32umls = parallel_mapping(atc3_id2emb, umls_ent_emb, num_processes, "ATC3->UMLS")
        
        # ç«‹å³ä¿å­˜ ATC3 æ˜ å°„ç»“æœ
        with open(atc3_file, 'wb') as f:
            pickle.dump(atc32umls, f)
        print(f"âœ“ ATC3->UMLS æ˜ å°„å·²ä¿å­˜: {len(atc32umls)} ä¸ªæ˜ å°„ -> {atc3_file}")
        del atc32umls  # é‡Šæ”¾å†…å­˜
    
    print("\nğŸ‰ æ‰€æœ‰æ˜ å°„ä»»åŠ¡å¤„ç†å®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - {ccscm_file}")
    print(f"   - {ccsproc_file}")
    print(f"   - {atc3_file}")
